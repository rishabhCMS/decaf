{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import lz4.frame as lz4f\n",
    "import cloudpickle\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import awkward\n",
    "np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "from coffea.arrays import Initialize\n",
    "from coffea import hist, processor\n",
    "from coffea.util import load, save\n",
    "from coffea.jetmet_tools import FactorizedJetCorrector, JetCorrectionUncertainty, JetTransformer, JetResolution, JetResolutionScaleFactor\n",
    "from optparse import OptionParser\n",
    "from uproot_methods import TVector2Array, TLorentzVectorArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from uproot_methods import TVector2Array, TLorentzVectorArray\n",
    "from optparse import OptionParser\n",
    "from coffea.jetmet_tools import FactorizedJetCorrector, JetCorrectionUncertainty, JetTransformer, JetResolution, JetResolutionScaleFactor\n",
    "from coffea.util import load, save\n",
    "from coffea import hist, processor\n",
    "from coffea.arrays import Initialize\n",
    "import lz4.frame as lz4f\n",
    "import cloudpickle\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import math\n",
    "import awkward\n",
    "np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "\n",
    "\n",
    "class AnalysisProcessor(processor.ProcessorABC):\n",
    "\n",
    "    lumis = {  # Values from https://twiki.cern.ch/twiki/bin/viewauth/CMS/PdmVAnalysisSummaryTable\n",
    "        '2016': 35.92,\n",
    "        '2017': 40.66,\n",
    "        '2018': 59.74\n",
    "    }\n",
    "\n",
    "    met_filter_flags = {\n",
    "\n",
    "        '2016': ['goodVertices',\n",
    "                 'globalSuperTightHalo2016Filter',\n",
    "                 'HBHENoiseFilter',\n",
    "                 'HBHENoiseIsoFilter',\n",
    "                 'EcalDeadCellTriggerPrimitiveFilter',\n",
    "                 'BadPFMuonFilter'\n",
    "                 ],\n",
    "\n",
    "        '2017': ['goodVertices',\n",
    "                 'globalSuperTightHalo2016Filter',\n",
    "                 'HBHENoiseFilter',\n",
    "                 'HBHENoiseIsoFilter',\n",
    "                 'EcalDeadCellTriggerPrimitiveFilter',\n",
    "                 'BadPFMuonFilter',\n",
    "                 'ecalBadCalibFilterV2'\n",
    "                 ],\n",
    "\n",
    "        '2018': ['goodVertices',\n",
    "                 'globalSuperTightHalo2016Filter',\n",
    "                 'HBHENoiseFilter',\n",
    "                 'HBHENoiseIsoFilter',\n",
    "                 'EcalDeadCellTriggerPrimitiveFilter',\n",
    "                 'BadPFMuonFilter',\n",
    "                 'ecalBadCalibFilterV2'\n",
    "                 ]\n",
    "    }\n",
    "\n",
    "    def __init__(self, year, xsec, corrections, ids, common):\n",
    "\n",
    "        self._columns = \"\"\"\n",
    "        CaloMET_pt\n",
    "        CaloMET_phi\n",
    "        Electron_charge\n",
    "        Electron_cutBased\n",
    "        Electron_dxy\n",
    "        Electron_dz\n",
    "        Electron_eta\n",
    "        Electron_mass\n",
    "        Electron_phi\n",
    "        Electron_pt\n",
    "        Flag_BadPFMuonFilter\n",
    "        Flag_EcalDeadCellTriggerPrimitiveFilter\n",
    "        Flag_HBHENoiseFilter\n",
    "        Flag_HBHENoiseIsoFilter\n",
    "        Flag_globalSuperTightHalo2016Filter\n",
    "        Flag_goodVertices\n",
    "        GenPart_eta\n",
    "        GenPart_genPartIdxMother\n",
    "        GenPart_pdgIdGenPart_phi\n",
    "        GenPart_pt\n",
    "        GenPart_statusFlags\n",
    "        HLT_Ele115_CaloIdVT_GsfTrkIdT\n",
    "        HLT_Ele32_WPTight_Gsf\n",
    "        HLT_PFMETNoMu120_PFMHTNoMu120_IDTight\n",
    "        HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60\n",
    "        HLT_Photon200\n",
    "        Jet_btagDeepB\n",
    "        Jet_btagDeepFlavB\n",
    "        Jet_chEmEF\n",
    "        Jet_chHEF\n",
    "        Jet_eta\n",
    "        Jet_hadronFlavour\n",
    "        Jet_jetId\n",
    "        Jet_mass\n",
    "        Jet_neEmEF\n",
    "        Jet_neHEF\n",
    "        Jet_phi\n",
    "        Jet_pt\n",
    "        Jet_rawFactor\n",
    "        MET_phi\n",
    "        MET_pt\n",
    "        Muon_charge\n",
    "        Muon_eta\n",
    "        Muon_looseId\n",
    "        Muon_mass\n",
    "        Muon_pfRelIso04_all\n",
    "        Muon_phi\n",
    "        Muon_pt\n",
    "        Muon_tightId\n",
    "        PV_npvs\n",
    "        Photon_eta\n",
    "        Photon_phi\n",
    "        Photon_pt\n",
    "        Tau_eta\n",
    "        Tau_idDecayMode\n",
    "        Tau_idMVAoldDM2017v2\n",
    "        Tau_phi\n",
    "        Tau_pt\n",
    "        fixedGridRhoFastjetAll\n",
    "        genWeight\n",
    "        nElectron\n",
    "        nGenPart\n",
    "        nJet\n",
    "        nMuon\n",
    "        nPhoton\n",
    "        nTau\n",
    "        \"\"\".split()\n",
    "\n",
    "        self._year = year\n",
    "\n",
    "        self._lumi = 1000.*float(AnalysisProcessor.lumis[year])\n",
    "\n",
    "        self._xsec = xsec\n",
    "        if self._year == '2016':\n",
    "            data_electron = 'SingleElectron'\n",
    "            data_muon = 'SingleMuon'\n",
    "        elif self._year == '2017':\n",
    "            data_electron = 'SingleElectron'\n",
    "            data_muon = 'SingleMuon'\n",
    "        elif self._year == '2018':\n",
    "            data_electron = 'EGamma'\n",
    "            data_muon = 'MET'\n",
    "        self._samples = {\n",
    "            'sre': ('WJets', 'DY', 'TT', 'ST', 'WW', 'WZ', 'ZZ', 'QCD', data_electron),\n",
    "            'srm': ('WJets', 'DY', 'TT', 'ST', 'WW', 'WZ', 'ZZ', 'QCD', data_muon),\n",
    "            'ttbare': ('WJets', 'DY', 'TT', 'ST', 'WW', 'WZ', 'ZZ', 'QCD', data_electron),\n",
    "            'ttbarm': ('WJets', 'DY', 'TT', 'ST', 'WW', 'WZ', 'ZZ', 'QCD', data_muon),\n",
    "            'wjete': ('WJets', 'DY', 'TT', 'ST', 'WW', 'WZ', 'ZZ', 'QCD', data_electron),\n",
    "            'wjetm': ('WJets', 'DY', 'TT', 'ST', 'WW', 'WZ', 'ZZ', 'QCD', data_muon),\n",
    "            #             'dilepe': ('DY', 'TT', 'ST', 'WW', 'WZ', 'ZZ', data_electron),\n",
    "            #             'dilepm': ('DY', 'TT', 'ST', 'WW', 'WZ', 'ZZ', data_muon)\n",
    "\n",
    "        }\n",
    "\n",
    "        self._gentype_map = {\n",
    "            'xbb':      1,\n",
    "            'tbcq':     2,\n",
    "            'tbqq':     3,\n",
    "            'zcc':      4,\n",
    "            'wcq':      5,\n",
    "            'vqq':      6,\n",
    "            'bb':       7,\n",
    "            'bc':       8,\n",
    "            'b':        9,\n",
    "            'cc':     10,\n",
    "            'c':       11,\n",
    "            'other':   12\n",
    "            # 'garbage': 13\n",
    "        }\n",
    "\n",
    "        self._ZHbbvsQCDwp = {\n",
    "            '2016': 0.53,\n",
    "            '2017': 0.61,\n",
    "            '2018': 0.65\n",
    "        }\n",
    "\n",
    "        self._met_triggers = {\n",
    "            '2016': [\n",
    "                'PFMETNoMu90_PFMHTNoMu90_IDTight',\n",
    "                'PFMETNoMu100_PFMHTNoMu100_IDTight',\n",
    "                'PFMETNoMu110_PFMHTNoMu110_IDTight',\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight'\n",
    "            ],\n",
    "            '2017': [\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60',\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight'\n",
    "            ],\n",
    "            '2018': [\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60',\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._singlephoton_triggers = {\n",
    "            '2016': [\n",
    "                'Photon175',\n",
    "                'Photon165_HE10'\n",
    "            ],\n",
    "            '2017': [\n",
    "                'Photon200'\n",
    "            ],\n",
    "            '2018': [\n",
    "                'Photon200'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._singleelectron_triggers = {  # 2017 and 2018 from monojet, applying dedicated trigger weights\n",
    "            '2016': [\n",
    "                'Ele27_WPTight_Gsf',\n",
    "                'Ele105_CaloIdVT_GsfTrkIdT'\n",
    "            ],\n",
    "            '2017': [\n",
    "                'Ele35_WPTight_Gsf',\n",
    "                'Ele115_CaloIdVT_GsfTrkIdT',\n",
    "                'Photon200'\n",
    "            ],\n",
    "            '2018': [\n",
    "                'Ele32_WPTight_Gsf',\n",
    "                'Ele115_CaloIdVT_GsfTrkIdT',\n",
    "                'Photon200'\n",
    "            ]\n",
    "        }\n",
    "        self._singlemuon_triggers = {\n",
    "            '2016': [\n",
    "                'IsoMu24',\n",
    "                'IsoTkMu24',\n",
    "                'Mu50',\n",
    "                'TkMu50'\n",
    "\n",
    "            ],\n",
    "            '2017':\n",
    "                [\n",
    "                'IsoMu27',\n",
    "                'Mu50',\n",
    "                'OldMu100',\n",
    "                'TkMu100'\n",
    "            ],\n",
    "            '2018':\n",
    "                [\n",
    "                'IsoMu24',\n",
    "                'Mu50',\n",
    "                'OldMu100',\n",
    "                'TkMu100'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._jec = {\n",
    "\n",
    "            '2016': [\n",
    "                'Summer16_07Aug2017_V11_MC_L1FastJet_AK4PFPuppi',\n",
    "                'Summer16_07Aug2017_V11_MC_L2L3Residual_AK4PFPuppi',\n",
    "                'Summer16_07Aug2017_V11_MC_L2Relative_AK4PFPuppi',\n",
    "                'Summer16_07Aug2017_V11_MC_L2Residual_AK4PFPuppi',\n",
    "                'Summer16_07Aug2017_V11_MC_L3Absolute_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2017': [\n",
    "                'Fall17_17Nov2017_V32_MC_L1FastJet_AK4PFPuppi',\n",
    "                'Fall17_17Nov2017_V32_MC_L2L3Residual_AK4PFPuppi',\n",
    "                'Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi',\n",
    "                'Fall17_17Nov2017_V32_MC_L2Residual_AK4PFPuppi',\n",
    "                'Fall17_17Nov2017_V32_MC_L3Absolute_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2018': [\n",
    "                'Autumn18_V19_MC_L1FastJet_AK4PFPuppi',\n",
    "                'Autumn18_V19_MC_L2L3Residual_AK4PFPuppi',\n",
    "                'Autumn18_V19_MC_L2Relative_AK4PFPuppi',  # currently broken\n",
    "                'Autumn18_V19_MC_L2Residual_AK4PFPuppi',\n",
    "                'Autumn18_V19_MC_L3Absolute_AK4PFPuppi'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._junc = {\n",
    "\n",
    "            '2016': [\n",
    "                'Summer16_07Aug2017_V11_MC_Uncertainty_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2017': [\n",
    "                'Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2018': [\n",
    "                'Autumn18_V19_MC_Uncertainty_AK4PFPuppi'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._jr = {\n",
    "\n",
    "            '2016': [\n",
    "                'Summer16_25nsV1b_MC_PtResolution_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2017': [\n",
    "                'Fall17_V3b_MC_PtResolution_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2018': [\n",
    "                'Autumn18_V7b_MC_PtResolution_AK4PFPuppi'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._jersf = {\n",
    "\n",
    "            '2016': [\n",
    "                'Summer16_25nsV1b_MC_SF_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2017': [\n",
    "                'Fall17_V3b_MC_SF_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2018': [\n",
    "                'Autumn18_V7b_MC_SF_AK4PFPuppi'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._corrections = corrections\n",
    "        self._ids = ids\n",
    "        self._common = common\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'sumw': hist.Hist(\n",
    "                'sumw',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Bin('sumw', 'Weight value', [0.])\n",
    "            ),\n",
    "            'template': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Cat('systematic', 'Systematic'),\n",
    "                hist.Bin('recoil', 'Hadronic Recoil', [\n",
    "                         250, 310, 370, 470, 590, 840, 1020, 1250, 3000]),\n",
    "            ),\n",
    "            'recoil': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('recoil', 'Hadronic Recoil', [250.0, 280.0, 310.0, 340.0, 370.0, 400.0, 430.0,\n",
    "                                                       470.0, 510.0, 550.0, 590.0,\n",
    "                                                       640.0, 690.0, 740.0, 790.0,\n",
    "                                                       840.0, 900.0, 960.0, 1020.0, 1090.0, 1160.0, 1250.0, 3000]),\n",
    "            ),\n",
    "            'mT': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('mT', '$m_{T}$ [GeV]', 20, 0, 600)),\n",
    "            'eT_miss': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('eT_miss', '$E^T_{miss}$[GeV]', 20, 0, 600)),\n",
    "\n",
    "            'ele_pT': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('ele_pT', 'Tight electron $p_{T}$ [GeV]', 10, 0, 200)),\n",
    "\n",
    "            'mu_pT': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('mu_pT', 'Tight Muon $p_{T}$ [GeV]', 10, 0, 200)),\n",
    "\n",
    "            'jet_pT': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('jet_pT', 'Leading $AK4 Jet p_{T}$ [GeV]',\n",
    "                         [30.0, 60.0, 90.0, 120.0, 150.0, 180.0, 210.0, 250.0, 280.0, 310.0, 340.0, 370.0, 400.0, 430.0, 470.0, 510.0, 550.0, 590.0, 640.0, 690.0, 740.0, 790.0, 840.0, 900.0, 960.0, 1020.0, 1090.0, 1160.0, 1250.0])),\n",
    "            'dphi_e_etmiss': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('dphi_e_etmiss', '$\\Delta\\phi (e, E^T_{miss} )$', 30, 0, 3.5)),\n",
    "            'dphi_mu_etmiss': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('dphi_mu_etmiss',\n",
    "                         '$\\Delta\\phi (\\mu, E^T_{miss} )$', 30, 0, 3.5)\n",
    "            ),\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "\n",
    "    def process(self, events):\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "\n",
    "        selected_regions = []\n",
    "        for region, samples in self._samples.items():\n",
    "            for sample in samples:\n",
    "                if sample not in dataset:\n",
    "                    continue\n",
    "                selected_regions.append(region)\n",
    "\n",
    "        isData = 'genWeight' not in events.columns\n",
    "        selection = processor.PackedSelection()\n",
    "        hout = self.accumulator.identity()\n",
    "\n",
    "        ###\n",
    "        # Getting corrections, ids from .coffea files\n",
    "        ###\n",
    "\n",
    "        get_msd_weight = self._corrections['get_msd_weight']\n",
    "        get_ttbar_weight = self._corrections['get_ttbar_weight']\n",
    "        get_nnlo_nlo_weight = self._corrections['get_nnlo_nlo_weight'][self._year]\n",
    "        get_nlo_qcd_weight      = self._corrections['get_nlo_qcd_weight'][self._year]\n",
    "        get_nlo_ewk_weight      = self._corrections['get_nlo_ewk_weight'][self._year]\n",
    "        get_pu_weight = self._corrections['get_pu_weight'][self._year]\n",
    "        get_met_trig_weight = self._corrections['get_met_trig_weight'][self._year]\n",
    "        get_met_zmm_trig_weight = self._corrections['get_met_zmm_trig_weight'][self._year]\n",
    "        get_ele_trig_weight = self._corrections['get_ele_trig_weight'][self._year]\n",
    "        get_pho_trig_weight = self._corrections['get_pho_trig_weight'][self._year]\n",
    "        get_ele_loose_id_sf = self._corrections['get_ele_loose_id_sf'][self._year]\n",
    "        get_ele_tight_id_sf = self._corrections['get_ele_tight_id_sf'][self._year]\n",
    "        get_pho_tight_id_sf = self._corrections['get_pho_tight_id_sf'][self._year]\n",
    "        get_pho_csev_sf = self._corrections['get_pho_csev_sf'][self._year]\n",
    "        get_mu_tight_id_sf = self._corrections['get_mu_tight_id_sf'][self._year]\n",
    "        get_mu_loose_id_sf = self._corrections['get_mu_loose_id_sf'][self._year]\n",
    "        get_ele_reco_sf = self._corrections['get_ele_reco_sf'][self._year]\n",
    "        get_ele_reco_lowet_sf = self._corrections['get_ele_reco_lowet_sf']\n",
    "        get_mu_tight_iso_sf = self._corrections['get_mu_tight_iso_sf'][self._year]\n",
    "        get_mu_loose_iso_sf = self._corrections['get_mu_loose_iso_sf'][self._year]\n",
    "        get_ecal_bad_calib = self._corrections['get_ecal_bad_calib']\n",
    "        get_deepflav_weight = self._corrections['get_btag_weight']['deepflav'][self._year]\n",
    "        Jetevaluator = self._corrections['Jetevaluator']\n",
    "\n",
    "        isLooseElectron = self._ids['isLooseElectron']\n",
    "        isTightElectron = self._ids['isTightElectron']\n",
    "        isLooseMuon = self._ids['isLooseMuon']\n",
    "        isTightMuon = self._ids['isTightMuon']\n",
    "        isLooseTau = self._ids['isLooseTau']\n",
    "        isLoosePhoton = self._ids['isLoosePhoton']\n",
    "        isTightPhoton = self._ids['isTightPhoton']\n",
    "        isGoodJet = self._ids['isGoodJet']\n",
    "        isHEMJet = self._ids['isHEMJet']\n",
    "\n",
    "        match = self._common['match']\n",
    "        # to calculate photon trigger efficiency\n",
    "        sigmoid = self._common['sigmoid']\n",
    "        deepflavWPs = self._common['btagWPs']['deepflav'][self._year]\n",
    "        deepcsvWPs = self._common['btagWPs']['deepcsv'][self._year]\n",
    "\n",
    "        ###\n",
    "        # Derive jet corrector for JEC/JER\n",
    "        ###\n",
    "\n",
    "        JECcorrector = FactorizedJetCorrector(\n",
    "            **{name: Jetevaluator[name] for name in self._jec[self._year]})\n",
    "        JECuncertainties = JetCorrectionUncertainty(\n",
    "            **{name: Jetevaluator[name] for name in self._junc[self._year]})\n",
    "        JER = JetResolution(\n",
    "            **{name: Jetevaluator[name] for name in self._jr[self._year]})\n",
    "        JERsf = JetResolutionScaleFactor(\n",
    "            **{name: Jetevaluator[name] for name in self._jersf[self._year]})\n",
    "        Jet_transformer = JetTransformer(\n",
    "            jec=JECcorrector, junc=JECuncertainties, jer=JER, jersf=JERsf)\n",
    "\n",
    "        ###\n",
    "        # Initialize global quantities (MET ecc.)\n",
    "        ###\n",
    "\n",
    "        met = events.MET\n",
    "        if self._year == '2017':\n",
    "            events.METFixEE2017  # Recommended for 2017\n",
    "        met['T'] = TVector2Array.from_polar(met.pt, met.phi)\n",
    "        calomet = events.CaloMET\n",
    "        puppimet = events.PuppiMET\n",
    "\n",
    "        ###\n",
    "        # Initialize physics objects\n",
    "        ###\n",
    "\n",
    "        mu = events.Muon\n",
    "        mu['isloose'] = isLooseMuon(\n",
    "            mu.pt, mu.eta, mu.pfRelIso04_all, mu.looseId, self._year)\n",
    "        mu['istight'] = isTightMuon(\n",
    "            mu.pt, mu.eta, mu.pfRelIso04_all, mu.tightId, self._year)\n",
    "        mu['T'] = TVector2Array.from_polar(mu.pt, mu.phi)\n",
    "        mu['p4'] = TLorentzVectorArray.from_ptetaphim(\n",
    "            mu.pt, mu.eta, mu.phi, mu.mass)\n",
    "        mu_loose = mu[mu.isloose.astype(np.bool)]\n",
    "        mu_tight = mu[mu.istight.astype(np.bool)]\n",
    "        mu_ntot = mu.counts\n",
    "        mu_nloose = mu_loose.counts\n",
    "        mu_ntight = mu_tight.counts\n",
    "        leading_mu = mu[mu.pt.argmax()]\n",
    "        leading_mu = leading_mu[leading_mu.istight.astype(np.bool)]\n",
    "\n",
    "        e = events.Electron\n",
    "        e['isclean'] = ~match(e, mu_loose, 0.3)\n",
    "        e['isloose'] = isLooseElectron(\n",
    "            e.pt, e.eta+e.deltaEtaSC, e.dxy, e.dz, e.cutBased, self._year)\n",
    "        e['istight'] = isTightElectron(\n",
    "            e.pt, e.eta+e.deltaEtaSC, e.dxy, e.dz, e.cutBased, self._year)\n",
    "        e['T'] = TVector2Array.from_polar(e.pt, e.phi)\n",
    "        e['p4'] = TLorentzVectorArray.from_ptetaphim(\n",
    "            e.pt, e.eta, e.phi, e.mass)\n",
    "        e_clean = e[e.isclean.astype(np.bool)]\n",
    "        e_loose = e_clean[e_clean.isloose.astype(np.bool)]\n",
    "        e_tight = e_clean[e_clean.istight.astype(np.bool)]\n",
    "        e_ntot = e.counts\n",
    "        e_nloose = e_loose.counts\n",
    "        e_ntight = e_tight.counts\n",
    "        leading_e = e[e.pt.argmax()]\n",
    "        leading_e = leading_e[leading_e.isclean.astype(np.bool)]\n",
    "        leading_e = leading_e[leading_e.istight.astype(np.bool)]\n",
    "\n",
    "        tau = events.Tau\n",
    "        tau['isclean'] = ~match(tau, mu_loose, 0.4) & ~match(tau, e_loose, 0.4)\n",
    "        tau['isloose'] = isLooseTau(\n",
    "            tau.pt, tau.eta, tau.idDecayMode, tau.idMVAoldDM2017v2, self._year)\n",
    "        tau_clean = tau[tau.isclean.astype(np.bool)]\n",
    "        tau_loose = tau_clean[tau_clean.isloose.astype(np.bool)]\n",
    "        tau_ntot = tau.counts\n",
    "        tau_nloose = tau_loose.counts\n",
    "\n",
    "        pho = events.Photon\n",
    "        pho['isclean'] = ~match(pho, mu_loose, 0.5) & ~match(pho, e_loose, 0.5)\n",
    "        _id = 'cutBasedBitmap'\n",
    "        if self._year == '2016':\n",
    "            _id = 'cutBased'\n",
    "        pho['isloose'] = isLoosePhoton(pho.pt, pho.eta, pho[_id], self._year) & (\n",
    "            pho.electronVeto)  # added electron veto flag\n",
    "        pho['istight'] = isTightPhoton(pho.pt, pho[_id], self._year) & (\n",
    "            pho.isScEtaEB) & (pho.electronVeto)  # tight photons are barrel only\n",
    "        pho['T'] = TVector2Array.from_polar(pho.pt, pho.phi)\n",
    "        pho_clean = pho[pho.isclean.astype(np.bool)]\n",
    "        pho_loose = pho_clean[pho_clean.isloose.astype(np.bool)]\n",
    "        pho_tight = pho_clean[pho_clean.istight.astype(np.bool)]\n",
    "        pho_ntot = pho.counts\n",
    "        pho_nloose = pho_loose.counts\n",
    "        pho_ntight = pho_tight.counts\n",
    "        leading_pho = pho[pho.pt.argmax()]\n",
    "        leading_pho = leading_pho[leading_pho.isclean.astype(np.bool)]\n",
    "        leading_pho = leading_pho[leading_pho.istight.astype(np.bool)]\n",
    "\n",
    "        j = events.Jet\n",
    "        j['isgood'] = isGoodJet(j.pt, j.eta, j.jetId, j.puId, j.neHEF, j.chHEF)\n",
    "        j['isHEM'] = isHEMJet(j.pt, j.eta, j.phi)\n",
    "        j['isclean'] = ~match(j, e_loose, 0.4) & ~match(\n",
    "            j, mu_loose, 0.4) & ~match(j, pho_loose, 0.4)\n",
    "#         print(j.isclean)\n",
    "\n",
    "#         j['isiso'] = ~match(j,j[j.pt.argmax()],0.4)\n",
    "        j['isdcsvL'] = (j.btagDeepB > deepcsvWPs['loose'])\n",
    "        j['isdflvL'] = (j.btagDeepFlavB > deepflavWPs['loose'])\n",
    "        j['isdflvM'] = (j.btagDeepFlavB > deepflavWPs['medium'])\n",
    "        j['T'] = TVector2Array.from_polar(j.pt, j.phi)\n",
    "        j['p4'] = TLorentzVectorArray.from_ptetaphim(\n",
    "            j.pt, j.eta, j.phi, j.mass)\n",
    "        j['ptRaw'] = j.pt * (1-j.rawFactor)\n",
    "        j['massRaw'] = j.mass * (1-j.rawFactor)\n",
    "        j['rho'] = j.pt.ones_like()*events.fixedGridRhoFastjetAll.array\n",
    "        j_good = j[j.isgood.astype(np.bool)]\n",
    "        j_clean = j[j.isclean.astype(np.bool)]\n",
    "#         print(j_c)\n",
    "#         j_iso = j_clean[j_clean.isiso.astype(np.bool)]\n",
    "#         j_iso=j_clean[j_clean.astype(np.bool)]  # Sunil changed\n",
    "        j_dcsvL = j_clean[j_clean.isdcsvL.astype(np.bool)]\n",
    "        j_dflvL = j_clean[j_clean.isdflvL.astype(np.bool)]\n",
    "        j_dflvM = j_clean[j_clean.isdflvM.astype(np.bool)]\n",
    "        j_HEM = j[j.isHEM.astype(np.bool)]\n",
    "        j_ntot = j.counts\n",
    "        j_ngood = j_good.counts\n",
    "        j_nclean = j_clean.counts\n",
    "#         j_niso=j_iso.counts\n",
    "        j_ndcsvL = j_dcsvL.counts\n",
    "        j_ndflvL = j_dflvL.counts\n",
    "        j_nHEM = j_HEM.counts\n",
    "        leading_j = j[j.pt.argmax()]\n",
    "        leading_j = leading_j[leading_j.isgood.astype(np.bool)]\n",
    "        leading_j = leading_j[leading_j.isclean.astype(np.bool)]\n",
    "        # *****btag\n",
    "        # https://twiki.cern.ch/twiki/bin/viewauth/CMS/BtagRecommendation102X#Supported_Algorithms_and_Operati\n",
    "        # medium     0.4184\n",
    "        btagWP_medium = 0.4184\n",
    "        Jet_btag_medium = j_clean[j_clean['btagDeepB'] > btagWP_medium]\n",
    "        ###\n",
    "        # Calculating derivatives\n",
    "        ###\n",
    "\n",
    "        # ************ calculate delta phi( leading ak4jet, met) > 1.5***********\n",
    "\n",
    "        j['T'] = TVector2Array.from_polar(j.pt, j.phi)\n",
    "        j['p4'] = TLorentzVectorArray.from_ptetaphim(\n",
    "            j.pt, j.eta, j.phi, j.mass)\n",
    "\n",
    "        Delta_Phi_Met_LJ = (met['T'].delta_phi(leading_j['T'].sum()) > 1.5)\n",
    "\n",
    "        # *******calculate deltaR( leading ak4jet, e/mu) < 3.4 *****\n",
    "        LJ_Ele = leading_j['p4'].cross(e_tight['p4'])\n",
    "        DeltaR_LJ_Ele = LJ_Ele.i0.delta_r(LJ_Ele.i1)\n",
    "        DeltaR_LJ_Ele_mask = (DeltaR_LJ_Ele < 3.4).any()\n",
    "\n",
    "        LJ_Mu = leading_j['p4'].cross(mu_tight['p4'])\n",
    "        DeltaR_LJ_Mu = LJ_Mu.i0.delta_r(LJ_Mu.i1)\n",
    "        DeltaR_LJ_Mu_mask = (DeltaR_LJ_Mu < 3.4).any()\n",
    "\n",
    "        ele_pairs = e_loose.distincts()\n",
    "        diele = ele_pairs.i0+ele_pairs.i1\n",
    "        diele['T'] = TVector2Array.from_polar(diele.pt, diele.phi)\n",
    "        leading_ele_pair = ele_pairs[diele.pt.argmax()]\n",
    "        leading_diele = diele[diele.pt.argmax()]\n",
    "\n",
    "        mu_pairs = mu_loose.distincts()\n",
    "        dimu = mu_pairs.i0+mu_pairs.i1\n",
    "        dimu['T'] = TVector2Array.from_polar(dimu.pt, dimu.phi)\n",
    "        leading_mu_pair = mu_pairs[dimu.pt.argmax()]\n",
    "        leading_dimu = dimu[dimu.pt.argmax()]\n",
    "\n",
    "        ###\n",
    "        # Calculate recoil and transverse mass\n",
    "        ###\n",
    "\n",
    "        u = {\n",
    "            'sre': met.T+leading_e.T.sum(),\n",
    "            'srm': met.T+leading_mu.T.sum(),\n",
    "            'ttbare': met.T+leading_e.T.sum(),\n",
    "            'ttbarm': met.T+leading_mu.T.sum(),\n",
    "            'wjete': met.T+leading_e.T.sum(),\n",
    "            'wjetm': met.T+leading_mu.T.sum(),\n",
    "            #             'dilepe': met.T+leading_diele.T.sum(),\n",
    "            #             'dilepm': met.T+leading_dimu.T.sum(),\n",
    "            # 'gcr'   : met.T+leading_pho.T.sum()\n",
    "        }\n",
    "\n",
    "        mT = {\n",
    "            'sre': np.sqrt(2*leading_e.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_e.T.sum())))),\n",
    "            'srm': np.sqrt(2*leading_mu.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_mu.T.sum())))),\n",
    "            'ttbare': np.sqrt(2*leading_e.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_e.T.sum())))),\n",
    "            'ttbarm': np.sqrt(2*leading_mu.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_mu.T.sum())))),\n",
    "            'wjete': np.sqrt(2*leading_e.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_e.T.sum())))),\n",
    "            'wjetm': np.sqrt(2*leading_mu.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_mu.T.sum()))))\n",
    "        }\n",
    "\n",
    "        LTe = leading_e.pt.sum() + met.pt\n",
    "        LTm = leading_mu.pt.sum() + met.pt\n",
    "        mT_misET = {\n",
    "            'sre': np.sqrt(2*leading_e.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_e.T.sum())))),\n",
    "            'srm': np.sqrt(2*leading_mu.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_mu.T.sum())))),\n",
    "            'ttbare': np.sqrt(2*leading_e.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_e.T.sum())))),\n",
    "            'ttbarm': np.sqrt(2*leading_mu.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_mu.T.sum())))),\n",
    "            'wjete': np.sqrt(2*leading_e.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_e.T.sum())))),\n",
    "            'wjetm': np.sqrt(2*leading_mu.pt.sum()*met.pt*(1-np.cos(met.T.delta_phi(leading_mu.T.sum()))))\n",
    "        }\n",
    "\n",
    "\n",
    "#         Lpe=leading_e.pt / u.mag * \\\n",
    "#             cos(abs(u[region].delta_phi(leading_e.T)).min())\n",
    "#         Lpm=leading_mu.pt / u.mag * \\\n",
    "#             cos(abs(u[region].delta_phi(leading_mu.T)).min())\n",
    "\n",
    "#         dPhi=abs(dPhiLepW)  # nickname for absolute dPhiLepW\n",
    "    # MT = sqrt(2*metp4.Pt()*tightLeps[0].pt * (1-cos(dPhiLepW)))   # mT_{W}\n",
    "\n",
    "        ###\n",
    "        # Calculating weights\n",
    "        ###\n",
    "        if not isData:\n",
    "\n",
    "            ###\n",
    "            # JEC/JER\n",
    "            ###\n",
    "\n",
    "            # j['ptGenJet'] = j.matched_gen.pt\n",
    "            # Jet_transformer.transform(j)\n",
    "\n",
    "            gen = events.GenPart\n",
    "\n",
    "            gen['isb'] = (abs(gen.pdgId) == 5) & gen.hasFlags(\n",
    "                ['fromHardProcess', 'isLastCopy'])\n",
    "\n",
    "            gen['isc'] = (abs(gen.pdgId) == 4) & gen.hasFlags(\n",
    "                ['fromHardProcess', 'isLastCopy'])\n",
    "\n",
    "            gen['isTop'] = (abs(gen.pdgId) == 6) & gen.hasFlags(\n",
    "                ['fromHardProcess', 'isLastCopy'])\n",
    "            genTops = gen[gen.isTop]\n",
    "            nlo = np.ones(events.size)\n",
    "            if('TTJets' in dataset):\n",
    "                nlo = np.sqrt(get_ttbar_weight(\n",
    "                    genTops[:, 0].pt.sum()) * get_ttbar_weight(genTops[:, 1].pt.sum()))\n",
    "\n",
    "            gen['isW'] = (abs(gen.pdgId) == 24) & gen.hasFlags(\n",
    "                ['fromHardProcess', 'isLastCopy'])\n",
    "            gen['isZ'] = (abs(gen.pdgId) == 23) & gen.hasFlags(\n",
    "                ['fromHardProcess', 'isLastCopy'])\n",
    "            gen['isA'] = (abs(gen.pdgId) == 22) & gen.hasFlags(\n",
    "                ['isPrompt', 'fromHardProcess', 'isLastCopy']) & (gen.status == 1)\n",
    "\n",
    "            ###\n",
    "            # Calculating gen photon dynamic isolation as in https://arxiv.org/pdf/1705.04664.pdf\n",
    "            ###\n",
    "\n",
    "            epsilon_0_dyn = 0.1\n",
    "            n_dyn = 1\n",
    "            gen['R_dyn'] = (91.1876/(gen.pt * np.sqrt(epsilon_0_dyn))) * \\\n",
    "                (gen.isA).astype(np.int) + \\\n",
    "                (-999)*(~gen.isA).astype(np.int)\n",
    "            gen['R_0_dyn'] = gen.R_dyn * \\\n",
    "                (gen.R_dyn < 1.0).astype(np.int) + \\\n",
    "                (gen.R_dyn >= 1.0).astype(np.int)\n",
    "\n",
    "            def isolation(R):\n",
    "                hadrons = gen[  # Stable hadrons not in NanoAOD, using quarks/glouns instead\n",
    "                    ((abs(gen.pdgId) <= 5) | (abs(gen.pdgId) == 21)) &\n",
    "                    gen.hasFlags(['fromHardProcess', 'isFirstCopy'])\n",
    "                ]\n",
    "                genhadrons = gen.cross(hadrons, nested=True)\n",
    "                hadronic_et = genhadrons.i1[(\n",
    "                    genhadrons.i0.delta_r(genhadrons.i1) <= R)].pt.sum()\n",
    "                return (hadronic_et <= (epsilon_0_dyn * gen.pt * np.power((1 - np.cos(R)) / (1 - np.cos(gen.R_0_dyn)), n_dyn))) | (hadrons.counts == 0)\n",
    "\n",
    "            isIsoA=gen.isA\n",
    "            iterations = 5.\n",
    "            for i in range(1, int(iterations) + 1):\n",
    "                isIsoA=isIsoA&isolation(gen.R_0_dyn*i/iterations)\n",
    "            gen['isIsoA']=isIsoA\n",
    "\n",
    "            #genWs = gen[gen.isW&(gen.pt>=100)]\n",
    "            genWs = gen[gen.isW] \n",
    "            genZs = gen[gen.isZ]\n",
    "            genDYs = gen[gen.isZ&(gen.mass>30)]\n",
    "            genIsoAs = gen[gen.isIsoA] \n",
    "\n",
    "            nnlo_nlo = {}\n",
    "            nlo_qcd = np.ones(events.size)\n",
    "            nlo_ewk = np.ones(events.size)\n",
    "            if('GJets' in dataset): \n",
    "                if self._year=='2016':\n",
    "                    nlo_qcd = get_nlo_qcd_weight['a'](genIsoAs.pt.max())\n",
    "                    nlo_ewk = get_nlo_ewk_weight['a'](genIsoAs.pt.max())\n",
    "                for systematic in get_nnlo_nlo_weight['a']:\n",
    "                    nnlo_nlo[systematic] = get_nnlo_nlo_weight['a'][systematic](genIsoAs.pt.max())*((genIsoAs.counts>0)&(genIsoAs.pt.max()>=290)) + \\\n",
    "                                           get_nnlo_nlo_weight['a'][systematic](290)*((genIsoAs.counts>0)&~(genIsoAs.pt.max()>=290)&(genIsoAs.pt.max()>=100)) + \\\n",
    "                                           (~((genIsoAs.counts>0)&(genIsoAs.pt.max()>=100))).astype(np.int)\n",
    "            elif('WJets' in dataset): \n",
    "                nlo_qcd = get_nlo_qcd_weight['w'](genWs.pt.max())\n",
    "                nlo_ewk = get_nlo_ewk_weight['w'](genWs.pt.max())\n",
    "                for systematic in get_nnlo_nlo_weight['w']:\n",
    "                    nnlo_nlo[systematic] = get_nnlo_nlo_weight['w'][systematic](genWs.pt.max())*((genWs.counts>0)&(genWs.pt.max()>=100)) + \\\n",
    "                                           (~((genWs.counts>0)&(genWs.pt.max()>=100))).astype(np.int)\n",
    "            elif('DY' in dataset): \n",
    "                nlo_qcd = get_nlo_qcd_weight['dy'](genDYs.pt.max())\n",
    "                nlo_ewk = get_nlo_ewk_weight['dy'](genDYs.pt.max())\n",
    "                for systematic in get_nnlo_nlo_weight['dy']:\n",
    "                    nnlo_nlo[systematic] = get_nnlo_nlo_weight['dy'][systematic](genDYs.pt.max())*((genDYs.counts>0)&(genDYs.pt.max()>=100)) + \\\n",
    "                                           (~((genDYs.counts>0)&(genDYs.pt.max()>=100))).astype(np.int)\n",
    "            elif('ZJets' in dataset): \n",
    "                nlo_qcd = get_nlo_qcd_weight['z'](genZs.pt.max())\n",
    "                nlo_ewk = get_nlo_ewk_weight['z'](genZs.pt.max())\n",
    "                for systematic in get_nnlo_nlo_weight['z']:\n",
    "                    nnlo_nlo[systematic] = get_nnlo_nlo_weight['z'][systematic](genZs.pt.max())*((genZs.counts>0)&(genZs.pt.max()>=100)) + \\\n",
    "                                           (~((genZs.counts>0)&(genZs.pt.max()>=100))).astype(np.int)\n",
    "\n",
    "            ###\n",
    "            # Calculate PU weight and systematic variations\n",
    "            ###\n",
    "\n",
    "            pu = get_pu_weight(events.PV.npvs)\n",
    "\n",
    "            ###\n",
    "            # Trigger efficiency weight\n",
    "            ###\n",
    "\n",
    "            e1sf = get_ele_trig_weight(leading_ele_pair.i0.eta.sum()+leading_ele_pair.i0.deltaEtaSC.sum(\n",
    "            ), leading_ele_pair.i0.pt.sum())*(leading_ele_pair.i0.pt.sum() > 40).astype(np.int)\n",
    "            e2sf = get_ele_trig_weight(leading_ele_pair.i1.eta.sum()+leading_ele_pair.i1.deltaEtaSC.sum(\n",
    "            ), leading_ele_pair.i1.pt.sum())*(leading_ele_pair.i1.pt.sum() > 40).astype(np.int)\n",
    "\n",
    "            if self._year == '2016':\n",
    "                sf = get_pho_trig_weight(leading_pho.pt.sum())\n",
    "            elif self._year == '2017':  # Sigmoid used for 2017 and 2018, values from monojet\n",
    "                sf = sigmoid(leading_pho.pt.sum(), 0.335, 217.91, 0.065, 0.996) / \\\n",
    "                    sigmoid(leading_pho.pt.sum(), 0.244, 212.34, 0.050, 1.000)\n",
    "                sf[np.isnan(sf) | np.isinf(sf)] == 1\n",
    "            elif self._year == '2018':\n",
    "                sf = sigmoid(leading_pho.pt.sum(), 1.022, 218.39, 0.086, 0.999) / \\\n",
    "                    sigmoid(leading_pho.pt.sum(), 0.301, 212.83, 0.062, 1.000)\n",
    "                sf[np.isnan(sf) | np.isinf(sf)] == 1\n",
    "\n",
    "            trig = {\n",
    "                'sre': get_met_trig_weight(leading_e.eta.sum(), leading_e.pt.sum()),\n",
    "                'srm': get_met_trig_weight(leading_mu.eta.sum(), leading_mu.pt.sum()),\n",
    "                'ttbare': get_met_trig_weight(leading_e.eta.sum(), leading_e.pt.sum()),\n",
    "                'ttbarm': get_met_trig_weight(leading_mu.eta.sum(), leading_mu.pt.sum()),\n",
    "                'wjete': get_met_trig_weight(leading_e.eta.sum(), leading_e.pt.sum()),\n",
    "                'wjetm': get_met_trig_weight(leading_mu.eta.sum(), leading_mu.pt.sum()),\n",
    "                #                 'dilepe' : get_met_trig_weight(leading_e.eta.sum(),leading_e.pt.sum()),\n",
    "                #                 'dilepm' : get_met_trig_weight(leading_mu.eta.sum(),leading_mu.pt.sum()),\n",
    "            }\n",
    "\n",
    "            ###\n",
    "            # Calculating electron and muon ID weights\n",
    "            ###\n",
    "\n",
    "            mueta = abs(leading_mu.eta.sum())\n",
    "            mu1eta = abs(leading_mu_pair.i0.eta.sum())\n",
    "            mu2eta = abs(leading_mu_pair.i1.eta.sum())\n",
    "            if self._year == '2016':\n",
    "                mueta = leading_mu.eta.sum()\n",
    "                mu1eta = leading_mu_pair.i0.eta.sum()\n",
    "                mu2eta = leading_mu_pair.i1.eta.sum()\n",
    "            if self._year == '2016':\n",
    "                sf = get_pho_tight_id_sf(\n",
    "                    leading_pho.eta.sum(), leading_pho.pt.sum())\n",
    "            else:  # 2017/2018 monojet measurement depends only on abs(eta)\n",
    "                sf = get_pho_tight_id_sf(abs(leading_pho.eta.sum()))\n",
    "\n",
    "            ids = {\n",
    "                'sre': get_ele_tight_id_sf(leading_e.eta.sum(), leading_e.pt.sum()),\n",
    "                'srm': get_mu_tight_id_sf(leading_mu.eta.sum(), leading_mu.pt.sum()),\n",
    "                'ttbare': get_ele_tight_id_sf(leading_e.eta.sum(), leading_e.pt.sum()),\n",
    "                'ttbarm': get_mu_tight_id_sf(leading_mu.eta.sum(), leading_mu.pt.sum()),\n",
    "                'wjete': get_ele_tight_id_sf(leading_e.eta.sum(), leading_e.pt.sum()),\n",
    "                'wjetm': get_mu_tight_id_sf(leading_mu.eta.sum(), leading_mu.pt.sum()),\n",
    "                #                 'dilepe': get_ele_loose_id_sf(leading_ele_pair.i0.eta.sum()+leading_ele_pair.i0.deltaEtaSC.sum(), leading_ele_pair.i0.pt.sum()) * get_ele_loose_id_sf(leading_ele_pair.i1.eta.sum()+leading_ele_pair.i1.deltaEtaSC.sum(), leading_ele_pair.i1.pt.sum()),\n",
    "                #                 'dilepm': get_mu_loose_id_sf(mu1eta, leading_mu_pair.i0.pt.sum()) * get_mu_loose_id_sf(mu2eta, leading_mu_pair.i1.pt.sum()),\n",
    "            }\n",
    "\n",
    "            ###\n",
    "            # Reconstruction weights for electrons\n",
    "            ###\n",
    "\n",
    "            # 2017 has separate weights for low/high pT (threshold at 20 GeV)\n",
    "            def ele_reco_sf(pt, eta):\n",
    "                return get_ele_reco_sf(eta, pt)*(pt > 20).astype(np.int) + get_ele_reco_lowet_sf(eta, pt)*(~(pt > 20)).astype(np.int)\n",
    "\n",
    "            if self._year == '2017':\n",
    "                sf = ele_reco_sf\n",
    "            else:\n",
    "                sf = get_ele_reco_sf\n",
    "\n",
    "            reco = {\n",
    "                'sre': sf(leading_e.eta.sum()+leading_e.deltaEtaSC.sum(), leading_e.pt.sum()),\n",
    "                'srm': np.ones(events.size),\n",
    "                'ttbare': sf(leading_e.eta.sum()+leading_e.deltaEtaSC.sum(), leading_e.pt.sum()),\n",
    "                'ttbarm': np.ones(events.size),\n",
    "                'wjete': sf(leading_e.eta.sum()+leading_e.deltaEtaSC.sum(), leading_e.pt.sum()),\n",
    "                'wjetm': np.ones(events.size),\n",
    "                #                 'dilepe': sf(leading_ele_pair.i0.eta.sum()+leading_ele_pair.i0.deltaEtaSC.sum(), leading_ele_pair.i0.pt.sum()) * sf(leading_ele_pair.i1.eta.sum()+leading_ele_pair.i1.deltaEtaSC.sum(), leading_ele_pair.i1.pt.sum()),\n",
    "                #                 'dilepm': np.ones(events.size)\n",
    "            }\n",
    "\n",
    "            ###\n",
    "            # Isolation weights for muons\n",
    "            ###\n",
    "\n",
    "            isolation = {\n",
    "                'sre': np.ones(events.size),\n",
    "                'srm': get_mu_tight_iso_sf(mueta, leading_mu.pt.sum()),\n",
    "                'ttbare': np.ones(events.size),\n",
    "                'ttbarm': get_mu_tight_iso_sf(mueta, leading_mu.pt.sum()),\n",
    "                'wjete': np.ones(events.size),\n",
    "                'wjetm': get_mu_tight_iso_sf(mueta, leading_mu.pt.sum()),\n",
    "                #                 'dilepm': get_mu_loose_iso_sf(mu1eta, leading_mu_pair.i0.pt.sum()) * get_mu_loose_iso_sf(mu2eta, leading_mu_pair.i1.pt.sum()),\n",
    "                #                 'dilepe': np.ones(events.size),\n",
    "            }\n",
    "\n",
    "            ###\n",
    "            # CSEV weight for photons: https://twiki.cern.ch/twiki/bin/view/CMS/EgammaIDRecipesRun2#Electron_Veto_CSEV_or_pixel_seed\n",
    "            ###\n",
    "\n",
    "            if self._year == '2016':\n",
    "                csev_weight = get_pho_csev_sf(\n",
    "                    abs(leading_pho.eta.sum()), leading_pho.pt.sum())\n",
    "            elif self._year == '2017':\n",
    "                csev_sf_index = 0.5*(leading_pho.isScEtaEB.sum()).astype(np.int)+3.5*(~(leading_pho.isScEtaEB.sum())).astype(\n",
    "                    np.int)+1*(leading_pho.r9.sum() > 0.94).astype(np.int)+2*(leading_pho.r9.sum() <= 0.94).astype(np.int)\n",
    "                csev_weight = get_pho_csev_sf(csev_sf_index)\n",
    "            elif self._year == '2018':\n",
    "                csev_weight = get_pho_csev_sf(\n",
    "                    leading_pho.pt.sum(), abs(leading_pho.eta.sum()))\n",
    "            csev_weight[csev_weight == 0] = 1\n",
    "\n",
    "            csev = {\n",
    "                'sre': np.ones(events.size),\n",
    "                'srm': np.ones(events.size),\n",
    "                'ttbare': np.ones(events.size),\n",
    "                'ttbarm': np.ones(events.size),\n",
    "                'wjete': np.ones(events.size),\n",
    "                'wjetm': np.ones(events.size),\n",
    "                #                 'dilepe': np.ones(events.size),\n",
    "                #                 'dilepm': np.ones(events.size),\n",
    "                #                 'gcr':  csev_weight\n",
    "            }\n",
    "\n",
    "            ###\n",
    "            # AK4 b-tagging weights\n",
    "            ###\n",
    "\n",
    "            btag = {}\n",
    "            btagUp = {}\n",
    "            btagDown = {}\n",
    "            btag['sre'],   btagUp['sre'],   btagDown['sre'] = get_deepflav_weight['medium'](\n",
    "                j_clean.pt, j_clean.eta, j_clean.hadronFlavour, '1')\n",
    "            btag['srm'],   btagUp['srm'],   btagDown['srm'] = get_deepflav_weight['medium'](\n",
    "                j_clean.pt, j_clean.eta, j_clean.hadronFlavour, '1')\n",
    "            btag['ttbare'], btagUp['ttbare'], btagDown['ttbare'] = get_deepflav_weight['medium'](\n",
    "                j_clean.pt, j_clean.eta, j_clean.hadronFlavour, '-1')\n",
    "            btag['ttbarm'], btagUp['ttbarm'], btagDown['ttbarm'] = get_deepflav_weight['medium'](\n",
    "                j_clean.pt, j_clean.eta, j_clean.hadronFlavour, '-1')\n",
    "            btag['wjete'], btagUp['wjete'], btagDown['wjete'] = get_deepflav_weight['medium'](\n",
    "                j_clean.pt, j_clean.eta, j_clean.hadronFlavour, '0')\n",
    "            btag['wjetm'], btagUp['wjetm'], btagDown['wjetm'] = get_deepflav_weight['medium'](\n",
    "                j_clean.pt, j_clean.eta, j_clean.hadronFlavour, '0')\n",
    "#             btag['dilepe'], btagUp['dilepe'], btagDown['dilepe']=np.ones(\n",
    "#                 events.size), np.ones(events.size), np.ones(events.size)\n",
    "#             btag['dilepm'], btagUp['dilepm'], btagDown['dilepm']=np.ones(\n",
    "#                 events.size), np.ones(events.size), np.ones(events.size)\n",
    "#             btag['gcr'],  btagUp['gcr'],  btagDown['gcr']=np.ones(\n",
    "#                 events.size), np.ones(events.size), np.ones(events.size)\n",
    "\n",
    "        ###\n",
    "        # Selections\n",
    "        ###\n",
    "\n",
    "        met_filters = np.ones(events.size, dtype=np.bool)\n",
    "        # this filter is recommended for data only\n",
    "        if isData:\n",
    "            met_filters = met_filters & events.Flag['eeBadScFilter']\n",
    "        for flag in AnalysisProcessor.met_filter_flags[self._year]:\n",
    "            met_filters = met_filters & events.Flag[flag]\n",
    "        selection.add('met_filters', met_filters)\n",
    "\n",
    "        triggers = np.zeros(events.size, dtype=np.bool)\n",
    "        for path in self._met_triggers[self._year]:\n",
    "            if path not in events.HLT.columns:\n",
    "                continue\n",
    "            triggers = triggers | events.HLT[path]\n",
    "        selection.add('met_triggers', triggers)\n",
    "\n",
    "        triggers = np.zeros(events.size, dtype=np.bool)\n",
    "        for path in self._singleelectron_triggers[self._year]:\n",
    "            if path not in events.HLT.columns:\n",
    "                continue\n",
    "            triggers = triggers | events.HLT[path]\n",
    "        selection.add('single_electron_triggers', triggers)\n",
    "\n",
    "        triggers = np.zeros(events.size, dtype=np.bool)\n",
    "        for path in self._singlephoton_triggers[self._year]:\n",
    "            if path not in events.HLT.columns:\n",
    "                continue\n",
    "            triggers = triggers | events.HLT[path]\n",
    "        selection.add('single_photon_triggers', triggers)\n",
    "\n",
    "        triggers = np.zeros(events.size, dtype=np.bool)\n",
    "        for path in self._singlemuon_triggers:\n",
    "            if path not in events.HLT.columns:\n",
    "                continue\n",
    "            triggers = triggers | events.HLT[path]\n",
    "        selection.add('single_muon_triggers', triggers)\n",
    "\n",
    "        noHEMj = np.ones(events.size, dtype=np.bool)\n",
    "        if self._year == '2018':\n",
    "            noHEMj = (j_nHEM == 0)\n",
    "\n",
    "        noHEMmet = np.ones(events.size, dtype=np.bool)\n",
    "        if self._year == '2018':\n",
    "            noHEMmet = (met.pt > 470) | (met.phi > -0.62) | (met.phi < -1.62)\n",
    "\n",
    "        '''\n",
    "        what the next 6 lines of code do:\n",
    "\n",
    "        main object is to exclude events from JetHt sample with W_pT b/w 70-100 GeV\n",
    "\n",
    "        events.metadata['dataset'] = 'WJetsToLNu_HT-100To200_TuneCP5_13TeV-madgraphMLM-pythia8____27_'\n",
    "        dataset = 'WJetsToLNu'\n",
    "\n",
    "        see if the 'HT' is in the name of the sample\n",
    "        so, it first goes to genpart,\n",
    "        figures out if the genlevel process is hardprocess and firstcopy and there are genlevel particle with\n",
    "        abs(pdgID)= 24\n",
    "\n",
    "        ad selects only those events for the pT of W was > 100 GeV\n",
    "\n",
    "        '''\n",
    "\n",
    "        # predeclration just in cas I don't want the filter\n",
    "        # selection.add(\"exclude_low_WpT_JetHT\", np.full(len(events), True))\n",
    "        if 'WJetsToLNu' in dataset:\n",
    "            if events.metadata['dataset'].split('-')[0].split('_')[1] == 'HT':\n",
    "                GenPart = events.GenPart\n",
    "                remove_overlap = (GenPart[GenPart.hasFlags(['fromHardProcess', 'isFirstCopy', 'isPrompt']) &\n",
    "                                          ((abs(GenPart.pdgId) == 24))].pt > 50).all()\n",
    "                selection.add(\"exclude_low_WpT_JetHT\", remove_overlap)\n",
    "\n",
    "        else:\n",
    "            selection.add(\"exclude_low_WpT_JetHT\", np.full(len(events), True))\n",
    "        # i dont think I need a photon trigger\n",
    "        #         triggers = np.zeros(events.size, dtype=np.bool)\n",
    "        #         for path in PhoTrigger:\n",
    "        #             if path not in events.HLT.columns: continue\n",
    "        #             triggers = triggers | events.HLT[path]\n",
    "        #         selection.add('pho_triggers', triggers)\n",
    "\n",
    "#         selection.add('DeltaR_LJ_mask',\n",
    "#                     (DeltaR_LJ_Ele_mask | DeltaR_LJ_Mu_mask))\n",
    "\n",
    "        selection.add('iszeroL', (e_nloose == 0) & (mu_nloose == 0)\n",
    "                      & (tau_nloose == 0) & (pho_nloose == 0))\n",
    "        selection.add('isoneM', (e_nloose == 0) & (mu_ntight == 1) & (\n",
    "            mu_nloose == 1) & (tau_nloose == 0) & (pho_nloose == 0))\n",
    "        selection.add('isoneE', (e_ntight == 1) & (e_nloose == 1) & (\n",
    "            mu_nloose == 0) & (tau_nloose == 0) & (pho_nloose == 0))\n",
    "        selection.add('istwoM', (e_nloose == 0) & (mu_nloose == 2)\n",
    "                      & (tau_nloose == 0) & (pho_nloose == 0))\n",
    "        selection.add('istwoE', (e_nloose == 2) & (mu_nloose == 0)\n",
    "                      & (tau_nloose == 0) & (pho_nloose == 0))\n",
    "        selection.add('isoneA', (e_nloose == 0) & (mu_nloose == 0) & (\n",
    "            tau_nloose == 0) & (pho_ntight == 1) & (pho_nloose == 1))\n",
    "        selection.add('leading_e_pt', (e_loose.pt.max() > 40))\n",
    "        selection.add('dimu_mass', (leading_dimu.mass.sum() > 60)\n",
    "                      & (leading_dimu.mass.sum() < 120))\n",
    "        selection.add('diele_mass', (leading_diele.mass.sum() > 60)\n",
    "                      & (leading_diele.mass.sum() < 120))\n",
    "        selection.add('noextrab', (j_ndflvM == 0))\n",
    "        selection.add('extrab', (j_ndflvM >= 2))\n",
    "        selection.add('oneb', (j_ndflvM == 1))\n",
    "        selection.add('noHEMj', noHEMj)\n",
    "        selection.add('noHEMmet', noHEMmet)\n",
    "        selection.add('met80', (met.pt < 80))\n",
    "        selection.add('met100', (met.pt > 100))\n",
    "        selection.add(\n",
    "            'mindphimet', (abs(met.T.delta_phi(j_clean.T)).min()) > 0.7)\n",
    "        # selection.add('zero_medium_btags',\n",
    "        #               (j_clean[j_clean['btagDeepB'] > btagWP_medium].counts == 0))\n",
    "        selection.add('Delta_Phi_Met_LJ', (Delta_Phi_Met_LJ))\n",
    "        selection.add('DeltaR_LJ_Ele', (DeltaR_LJ_Ele_mask))\n",
    "\n",
    "        selection.add('one_muon', (mu_tight.counts == 1))\n",
    "        selection.add('zero_loose_electron', (e_loose.counts == 0))\n",
    "        selection.add('DeltaR_LJ_Mu', (DeltaR_LJ_Mu_mask))\n",
    "\n",
    "        # selection.add('atleast_2_medium_btag',\n",
    "        #               (j_clean[j_clean['btagDeepB'] > btagWP_medium].counts >= 2))\n",
    "\n",
    "        # selection.add('exactly_1_medium_btag',\n",
    "        #               (j_clean[j_clean['btagDeepB'] > btagWP_medium].counts == 1))\n",
    "        regions = {\n",
    "            'sre': {'isoneE', 'oneb', 'noHEMj', 'met_filters', 'single_electron_triggers', 'met100', 'exclude_low_WpT_JetHT',\n",
    "                    'Delta_Phi_Met_LJ',\n",
    "                    'DeltaR_LJ_Ele'\n",
    "                    },\n",
    "            'srm': {'isoneM', 'oneb', 'noHEMj', 'met_filters', 'single_muon_triggers', 'met100', 'exclude_low_WpT_JetHT',\n",
    "                    'Delta_Phi_Met_LJ',\n",
    "                    'DeltaR_LJ_Mu'\n",
    "                    },\n",
    "            'ttbare': {'isoneE', 'extrab', 'noHEMj', 'met_filters', 'single_electron_triggers', 'met100', 'exclude_low_WpT_JetHT',\n",
    "                       'Delta_Phi_Met_LJ',\n",
    "                       'DeltaR_LJ_Ele'\n",
    "                       },\n",
    "            'ttbarm': {'isoneM', 'extrab', 'noHEMj', 'met_filters', 'single_muon_triggers', 'met100', 'exclude_low_WpT_JetHT',\n",
    "                       'Delta_Phi_Met_LJ',\n",
    "                       'DeltaR_LJ_Mu'\n",
    "                       },\n",
    "            'wjete': {'isoneE', 'noextrab', 'noHEMj', 'met_filters', 'single_electron_triggers', 'met100', 'exclude_low_WpT_JetHT',\n",
    "                      'Delta_Phi_Met_LJ',\n",
    "                      'DeltaR_LJ_Ele'\n",
    "                      },\n",
    "            'wjetm': {'isoneM', 'noextrab', 'noHEMj', 'met_filters', 'single_muon_triggers', 'met100', 'exclude_low_WpT_JetHT',\n",
    "                      'Delta_Phi_Met_LJ',\n",
    "                      'DeltaR_LJ_Mu'\n",
    "                      },\n",
    "            # 'dilepe' : {'istwoE','onebjet','noHEMj','met_filters','single_electron_triggers', 'met100', 'exclude_low_WpT_JetHT',\n",
    "            #             'Delta_Phi_Met_LJ', 'DeltaR_LJ_Ele'},\n",
    "            # 'dilepm' : {'istwoM','onebjet','noHEMj','met_filters','single_mu_triggers', 'met100', 'exclude_low_WpT_JetHT',\n",
    "            #             'Delta_Phi_Met_LJ', 'DeltaR_LJ_Mu'},\n",
    "\n",
    "            # 'gcr': {'isoneA','fatjet','noHEMj','met_filters','singlephoton_triggers'}\n",
    "        }\n",
    "\n",
    "        isFilled = False\n",
    "        # print(\"mu_ntight->\", mu_ntight.sum(),\n",
    "        #       '\\n', 'e_ntight->', e_ntight.sum())\n",
    "        for region in selected_regions:\n",
    "            #             print('Considering region:', region)\n",
    "\n",
    "            ###\n",
    "            # Adding recoil and minDPhi requirements\n",
    "            ###\n",
    "\n",
    "            # selection.add('recoil_'+region, (u[region].mag>250))\n",
    "            # selection.add('mindphi_'+region, (abs(u[region].delta_phi(j_clean.T)).min()>0.8))\n",
    "            # regions[region].update({'recoil_'+region,'mindphi_'+region})\n",
    "            #             print('Selection:',regions[region])\n",
    "            variables = {\n",
    "                'dphi_e_etmiss':          met['T'].delta_phi(leading_e['T'].sum()),\n",
    "                'dphi_mu_etmiss':         met['T'].delta_phi(leading_mu['T'].sum()),\n",
    "                'mu_pT':                  mu_tight.pt,\n",
    "                'recoil':                 u[region].mag,\n",
    "                # 'mindphirecoil':          abs(u[region].delta_phi(j_clean.T)).min(),\n",
    "                # 'CaloMinusPfOverRecoil':  abs(calomet.pt - met.pt) / u[region].mag,\n",
    "                'eT_miss':                met.pt,\n",
    "                'ele_pT':                 e_tight.pt,\n",
    "                'jet_pT':                 leading_j.pt\n",
    "                # 'metphi':                 met.phi,\n",
    "                # 'mindphimet':             abs(met.T.delta_phi(j_clean.T)).min(),\n",
    "                # 'j1pt':                   leading_j.pt,\n",
    "                # 'j1eta':                  leading_j.eta,\n",
    "                # 'j1phi':                  leading_j.phi,\n",
    "                # 'njets':                  j_nclean,\n",
    "                # 'ndflvL':                 j_ndflvL,\n",
    "                # 'ndcsvL':     j_ndcsvL,\n",
    "                # 'e1pt'      : leading_e.pt,\n",
    "                # 'e1phi'     : leading_e.phi,\n",
    "                # 'e1eta'     : leading_e.eta,\n",
    "                # 'dielemass' : leading_diele.mass,\n",
    "                # 'dielept'   : leading_diele.pt,\n",
    "                # 'mu1pt' : leading_mu.pt,\n",
    "                # 'mu1phi' : leading_mu.phi,\n",
    "                # 'mu1eta' : leading_mu.eta,\n",
    "                # 'dimumass' : leading_dimu.mass,\n",
    "                # 'dimupt' : leading_dimu.pt\n",
    "            }\n",
    "            if region in mT:\n",
    "                variables['mT'] = mT[region]\n",
    "                if 'e' in region:\n",
    "                    WRF = leading_e.T.sum()-met.T\n",
    "                else:\n",
    "                    pass\n",
    "#                     WRF = leading_mu.T.sum()-met.T\n",
    "#                 variables['recoilphiWRF'] = abs(u[region].delta_phi(WRF))\n",
    "            print('Variables:', variables.keys())\n",
    "\n",
    "            def fill(dataset, weight, cut):\n",
    "\n",
    "                flat_variables = {k: v[cut].flatten()\n",
    "                                  for k, v in variables.items()}\n",
    "                flat_weight = {\n",
    "                    k: (~np.isnan(v[cut])*weight[cut]).flatten() for k, v in variables.items()}\n",
    "\n",
    "                for histname, h in hout.items():\n",
    "                    if not isinstance(h, hist.Hist):\n",
    "                        continue\n",
    "                    if histname not in variables:\n",
    "                        continue\n",
    "                    elif histname == 'sumw':\n",
    "                        continue\n",
    "                    elif histname == 'template':\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(histname)\n",
    "                        flat_variable = {histname: flat_variables[histname]}\n",
    "                        h.fill(dataset=dataset,\n",
    "                               region=region,\n",
    "                               **flat_variable,\n",
    "                               weight=flat_weight[histname])\n",
    "\n",
    "            if isData:\n",
    "                if not isFilled:\n",
    "                    hout['sumw'].fill(dataset=dataset, sumw=1, weight=1)\n",
    "                    isFilled = True\n",
    "                cut = selection.all(*regions[region])\n",
    "                hout['template'].fill(dataset=dataset,\n",
    "                                      region=region,\n",
    "                                      systematic='nominal',\n",
    "                                      recoil=u[region].mag,\n",
    "                                      weight=np.ones(events.size)*cut)\n",
    "                fill(dataset, np.ones(events.size), cut)\n",
    "            else:\n",
    "                weights = processor.Weights(len(events))\n",
    "                if 'L1PreFiringWeight' in events.columns: weights.add('prefiring',events.L1PreFiringWeight.Nom)\n",
    "                weights.add('genw',events.genWeight)\n",
    "                weights.add('nlo_qcd',nlo_qcd)\n",
    "                weights.add('nlo_ewk',nlo_ewk)\n",
    "                if 'cen' in nnlo_nlo:\n",
    "                    weights.add('nnlo_nlo',nnlo_nlo['cen'])\n",
    "                    weights.add('qcd1',np.ones(events.size), nnlo_nlo['qcd1up']/nnlo_nlo['cen'], nnlo_nlo['qcd1do']/nnlo_nlo['cen'])\n",
    "                    weights.add('qcd2',np.ones(events.size), nnlo_nlo['qcd2up']/nnlo_nlo['cen'], nnlo_nlo['qcd2do']/nnlo_nlo['cen'])\n",
    "                    weights.add('qcd3',np.ones(events.size), nnlo_nlo['qcd3up']/nnlo_nlo['cen'], nnlo_nlo['qcd3do']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew1',np.ones(events.size), nnlo_nlo['ew1up']/nnlo_nlo['cen'], nnlo_nlo['ew1do']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew2G',np.ones(events.size), nnlo_nlo['ew2Gup']/nnlo_nlo['cen'], nnlo_nlo['ew2Gdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew3G',np.ones(events.size), nnlo_nlo['ew3Gup']/nnlo_nlo['cen'], nnlo_nlo['ew3Gdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew2W',np.ones(events.size), nnlo_nlo['ew2Wup']/nnlo_nlo['cen'], nnlo_nlo['ew2Wdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew3W',np.ones(events.size), nnlo_nlo['ew3Wup']/nnlo_nlo['cen'], nnlo_nlo['ew3Wdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew2Z',np.ones(events.size), nnlo_nlo['ew2Zup']/nnlo_nlo['cen'], nnlo_nlo['ew2Zdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew3Z',np.ones(events.size), nnlo_nlo['ew3Zup']/nnlo_nlo['cen'], nnlo_nlo['ew3Zdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('mix',np.ones(events.size), nnlo_nlo['mixup']/nnlo_nlo['cen'], nnlo_nlo['mixdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('muF',np.ones(events.size), nnlo_nlo['muFup']/nnlo_nlo['cen'], nnlo_nlo['muFdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('muR',np.ones(events.size), nnlo_nlo['muRup']/nnlo_nlo['cen'], nnlo_nlo['muRdo']/nnlo_nlo['cen'])\n",
    "                weights.add('pileup',pu)\n",
    "                weights.add('trig', trig[region])\n",
    "                weights.add('ids', ids[region])\n",
    "                weights.add('reco', reco[region])\n",
    "                weights.add('isolation', isolation[region])\n",
    "                weights.add('csev', csev[region])\n",
    "                weights.add('btag',btag[region], btagUp[region], btagDown[region])\n",
    "\n",
    "                if 'WJets' in dataset or 'ZJets' in dataset or 'DY' in dataset or 'GJets' in dataset:\n",
    "                    if not isFilled:\n",
    "                        hout['sumw'].fill(\n",
    "                            dataset='HF--'+dataset, sumw=1, weight=events.genWeight.sum())\n",
    "                        hout['sumw'].fill(\n",
    "                            dataset='LF--'+dataset, sumw=1, weight=events.genWeight.sum())\n",
    "                        isFilled = True\n",
    "                    whf = ((gen[gen.isb].counts > 0) | (\n",
    "                        gen[gen.isc].counts > 0)).astype(np.int)\n",
    "                    wlf = (~(whf.astype(np.bool))).astype(np.int)\n",
    "                    cut = selection.all(*regions[region])\n",
    "                    systematics = [None,\n",
    "                                   'btagUp',\n",
    "                                   'btagDown',\n",
    "                                   'qcd1Up',\n",
    "                                   'qcd1Down',\n",
    "                                   'qcd2Up',\n",
    "                                   'qcd2Down',\n",
    "                                   'qcd3Up',\n",
    "                                   'qcd3Down',\n",
    "                                   'muFUp',\n",
    "                                   'muFDown',\n",
    "                                   'muRUp',\n",
    "                                   'muRDown',\n",
    "                                   'ew1Up',\n",
    "                                   'ew1Down',\n",
    "                                   'ew2GUp',\n",
    "                                   'ew2GDown',\n",
    "                                   'ew2WUp',\n",
    "                                   'ew2WDown',\n",
    "                                   'ew2ZUp',\n",
    "                                   'ew2ZDown',\n",
    "                                   'ew3GUp',\n",
    "                                   'ew3GDown',\n",
    "                                   'ew3WUp',\n",
    "                                   'ew3WDown',\n",
    "                                   'ew3ZUp',\n",
    "                                   'ew3ZDown',\n",
    "                                   'mixUp',\n",
    "                                   'mixDown']\n",
    "                    for systematic in systematics:\n",
    "                        sname = 'nominal' if systematic is None else systematic\n",
    "                        hout['template'].fill(dataset='HF--'+dataset,\n",
    "                                              region=region,\n",
    "                                              systematic=sname,\n",
    "                                              recoil=u[region].mag,\n",
    "                                              weight=weights.weight(modifier=systematic)*whf*cut)\n",
    "                        hout['template'].fill(dataset='LF--'+dataset,\n",
    "                                              region=region,\n",
    "                                              systematic=sname,\n",
    "                                              recoil=u[region].mag,\n",
    "                                              weight=weights.weight(modifier=systematic)*wlf*cut)\n",
    "                    fill('HF--'+dataset, weights.weight()*whf, cut)\n",
    "                    fill('LF--'+dataset, weights.weight()*wlf, cut)\n",
    "                else:\n",
    "                    if not isFilled:\n",
    "                        hout['sumw'].fill(\n",
    "                            dataset=dataset, sumw=1, weight=events.genWeight.sum())\n",
    "                        isFilled = True\n",
    "                    cut = selection.all(*regions[region])\n",
    "                    for systematic in [None, 'btagUp', 'btagDown']:\n",
    "                        sname = 'nominal' if systematic is None else systematic\n",
    "                        hout['template'].fill(dataset=dataset,\n",
    "                                              region=region,\n",
    "                                              systematic=sname,\n",
    "                                              recoil=u[region].mag,\n",
    "                                              weight=weights.weight(modifier=systematic)*cut)\n",
    "\n",
    "                    fill(dataset, weights.weight(), cut)\n",
    "\n",
    "        return hout\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        scale = {}\n",
    "        for d in accumulator['sumw'].identifiers('dataset'):\n",
    "            print('Scaling:', d.name)\n",
    "            dataset = d.name\n",
    "            if '--' in dataset:\n",
    "                dataset = dataset.split('--')[1]\n",
    "            print('Cross section:', self._xsec[dataset])\n",
    "            if self._xsec[dataset] != -1:\n",
    "                scale[d.name] = self._lumi*self._xsec[dataset]\n",
    "            else:\n",
    "                scale[d.name] = 1\n",
    "\n",
    "        for histname, h in accumulator.items():\n",
    "            if histname == 'sumw':\n",
    "                continue\n",
    "            if isinstance(h, hist.Hist):\n",
    "                h.scale(scale, axis='dataset')\n",
    "\n",
    "        return accumulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e6905035ce4c048a057eb66e34970a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Processing', max=1.0, style=ProgressStyle(description_wid"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photon K fac: [1.33520354 1.33520354 1.33520354 ... 1.33520354 1.33520354 1.33520354] \n",
      "\n",
      "W K fact: [1.40777751 1.40777751 1.40777751 ... 1.40777751 1.40777751 1.40777751] \n",
      "\n",
      "DY K fac: [1.51607085 1.51607085 1.51607085 ... 1.51607085 1.51607085 1.51607085] \n",
      "\n",
      "Z k fac [1.49940686 1.49940686 1.49940686 ... 1.49940686 1.49940686 1.49940686] \n",
      "\n",
      "\n",
      " <class 'numpy.ndarray'>\n",
      "\n",
      " 23306\n",
      "events.size:23306\n",
      "events.genWeight len:23306\n",
      "len nlo_qcd:23306\n",
      "\n",
      "yields\n"
     ]
    }
   ],
   "source": [
    "fileset = {'WJetsToLNu_HT-100To200_TuneCP5_13TeV-madgraphMLM-pythia8____1_':\n",
    "[\n",
    "            \"root://cmseos.fnal.gov//store/group/lpccoffea/coffeabeans/NanoAODv6/nano_2018/WJetsToLNu_HT-100To200_TuneCP5_13TeV-madgraphMLM-pythia8/NanoTuples-2018_RunIIAutumn18MiniAOD-102X_v15-v1/191207_092936/0000/nano_128.root\",\n",
    "#             \"root://cmseos.fnal.gov//store/group/lpccoffea/coffeabeans/NanoAODv6/nano_2018/WJetsToLNu_HT-100To200_TuneCP5_13TeV-madgraphMLM-pythia8/NanoTuples-2018_RunIIAutumn18MiniAOD-102X_v15-v1/191207_092936/0000/nano_129.root\",\n",
    "        ],\n",
    "\n",
    "#          'MET____0_'  :\n",
    "#            [\n",
    "#             \"root://cmseos.fnal.gov//store/group/lpccoffea/coffeabeans/NanoAODv6/nano_2018/MET/NanoTuples-2018_Run2018A-17Sep2018-v1/191206_204012/0000/nano_126.root\",\n",
    "#             \"root://cmseos.fnal.gov//store/group/lpccoffea/coffeabeans/NanoAODv6/nano_2018/MET/NanoTuples-2018_Run2018A-17Sep2018-v1/191206_204012/0000/nano_127.root\"\n",
    "#         ]\n",
    "}\n",
    "#Run Coffea code using uproot\n",
    "with open('../metadata/2018.json') as fin:\n",
    "    samplefiles = json.load(fin)\n",
    "    xsec = {k: v['xs'] for k, v in samplefiles.items()}\n",
    "    \n",
    "corrections = load('/uscms_data/d3/runiyal/decaf_oct20/decaf/analysis/data/corrections.coffea')\n",
    "output = processor.run_uproot_job(fileset,\n",
    "                                  treename='Events',\n",
    "                                  processor_instance=AnalysisProcessor('2018', \n",
    "                                                                       xsec=xsec,\n",
    "                                                                       corrections=corrections),\n",
    "                                  executor=processor.futures_executor,\n",
    "                                  executor_args={'workers': 4, 'flatten': True, 'nano':True},\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = load('/uscms_data/d3/runiyal/decaf_oct20/decaf/analysis/data/corrections.coffea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['get_msd_weight', 'get_ttbar_weight', 'get_nnlo_nlo_weight', 'get_nlo_qcd_weight', 'get_nlo_ewk_weight', 'get_pu_weight', 'get_met_trig_weight', 'get_met_zmm_trig_weight', 'get_ele_trig_weight', 'get_pho_trig_weight', 'get_ele_loose_id_sf', 'get_ele_tight_id_sf', 'get_pho_tight_id_sf', 'get_pho_csev_sf', 'get_mu_tight_id_sf', 'get_mu_loose_id_sf', 'get_ele_reco_sf', 'get_ele_reco_lowet_sf', 'get_mu_tight_iso_sf', 'get_mu_loose_iso_sf', 'get_ecal_bad_calib', 'get_btag_weight', 'Jetevaluator'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nnlo_nlo_weight = corrections['get_nlo_ewk_weight']['2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dy', 'w', 'z', 'a'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nnlo_nlo_weight.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1 dimensional histogram with axes:\n",
       "\t1: [  30.   40.   50.   60.   70.   80.   90.  100.  110.  120.  130.  140.\n",
       "  150.  200.  250.  300.  350.  400.  450.  500.  550.  600.  650.  700.\n",
       "  750.  800.  850.  900.  950. 1000. 1100. 1200. 1300. 1400. 1600. 1800.\n",
       " 2000. 2200. 2400. 2600. 2800. 3000. 6500.]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections['get_nnlo_nlo_weight']['2018']['dy']['qcd1up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-87228565eafe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlst\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,) (3,) "
     ]
    }
   ],
   "source": [
    "lst*[1,0,1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
